{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Investigating textual similarities between Homer and Plato's collected works\n",
    "\n",
    "part 2 contains other similarity measures"
   ],
   "id": "da7669901198aadf"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "from datasketch import MinHash\n",
    "import unicodedata\n",
    "from gensim.models import FastText\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Util Functions",
   "id": "fd37d9e7fff0321"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def make_sentences_str(text):\n",
    "    \"\"\"\n",
    "    Turns a text in a list of sentences. Note that Ancient Greek uses different punctuation! \n",
    "    :param text: str\n",
    "    :return: list of strings\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    sentence = \"\"\n",
    "    \n",
    "    text = text.replace('.', ' .').replace(';', ' ;')\n",
    "    tokens = text.split()\n",
    "    for word in tokens:\n",
    "        if word in [';', '.']:\n",
    "            if sentence:\n",
    "                sentences.append(sentence[:-1])\n",
    "                sentence = \"\"\n",
    "        else:\n",
    "            sentence += word+\" \"\n",
    "    return sentences"
   ],
   "id": "3b8fad0986077265",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def make_sentences_list(text):\n",
    "    \"\"\"\n",
    "    Turns a text in a list of sentences. Note that Ancient Greek uses different punctuation! \n",
    "    :param text: str\n",
    "    :return: list of lists\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    \n",
    "    text = text.replace('.', ' .').replace(';', ' ;')\n",
    "    tokens = text.split()\n",
    "    for word in tokens:\n",
    "        if word in [';', '.']:\n",
    "            if sentence:\n",
    "                sentences.append(sentence[:-1])\n",
    "                sentence = []\n",
    "        else:\n",
    "            sentence.append(word)\n",
    "    return sentences"
   ],
   "id": "8c341431f0f824c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_sentence_pairs_above_threshold(df, sentences1, sentences2, threshold):\n",
    "    \"\"\"Get all sentence pairs with similarity above the given threshold.\"\"\"\n",
    "    pairs = []\n",
    "    for i, row in df.iterrows():\n",
    "        for j, similarity in row.items():\n",
    "            if similarity > threshold:\n",
    "                pairs.append({\n",
    "                    \"sentence1\": sentences1[i],\n",
    "                    \"sentence2\": sentences2[j],\n",
    "                    \"similarity\": similarity\n",
    "                })\n",
    "    return pairs"
   ],
   "id": "5ce00607ffd55547",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Comparison TF/IDF",
   "id": "d7e2cf62e692aed1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def calculate_tfidf_similarity(text1, text2):\n",
    "    sentences1 = make_sentences_str(text1)\n",
    "    sentences2 = make_sentences_str(text2)\n",
    "    \n",
    "    all_sentences = sentences1 + sentences2\n",
    "    \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(all_sentences)\n",
    "    \n",
    "    tfidf1 = tfidf_matrix[:len(sentences1)]\n",
    "    tfidf2 = tfidf_matrix[len(sentences1):]\n",
    "\n",
    "    similarity_matrix = cosine_similarity(tfidf1, tfidf2)\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        similarity_matrix,\n",
    "        index=range(len(sentences1)),\n",
    "        columns=range(len(sentences2))\n",
    "    )\n",
    "\n",
    "    return df, sentences1, sentences2"
   ],
   "id": "2c1aef758301dd3d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Comparison using FastText",
   "id": "3fa7530bf8e3b32b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def calculate_fasttext_similarity(text1, text2):\n",
    "    sentences1 = make_sentences_list(text1)\n",
    "    sentences2 = make_sentences_list(text2)\n",
    "    \n",
    "    model = FastText.load('fasttext_model.bin')\n",
    "    keyed_vectors = model.wv\n",
    "    vector_size = model.vector_size\n",
    "\n",
    "    def get_sentence_embedding(sentence):\n",
    "        if not sentence:  # Empty sentence\n",
    "            return np.zeros(vector_size)\n",
    "        \n",
    "        vectors = []\n",
    "        for word in sentence:\n",
    "            normalized_word = unicodedata.normalize(\"NFC\", word)\n",
    "            if normalized_word in keyed_vectors:\n",
    "                vectors.append(keyed_vectors[normalized_word])\n",
    "        \n",
    "        if vectors:\n",
    "            return sum(vectors) / len(vectors)\n",
    "        else:\n",
    "            return np.zeros(vector_size)\n",
    "\n",
    "    embeddings1 = [get_sentence_embedding(sentence) for sentence in sentences1]\n",
    "    embeddings2 = [get_sentence_embedding(sentence) for sentence in sentences2]\n",
    "\n",
    "    similarity_matrix = cosine_similarity(embeddings1, embeddings2)\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        similarity_matrix,\n",
    "        index=range(len(sentences1)),\n",
    "        columns=range(len(sentences2))\n",
    "    )\n",
    "\n",
    "    return df, sentences1, sentences2"
   ],
   "id": "c6367a485e57d03",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Comparison using MinHash",
   "id": "6012d96df2335fb3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def calculate_minhash_similarity(text1, text2):\n",
    "    sentences1 = [sentence for sentence in make_sentences_list(text1) if sentence]\n",
    "    sentences2 = [sentence for sentence in make_sentences_list(text2) if sentence]\n",
    "\n",
    "    if not sentences1 or not sentences2:\n",
    "        empty_df = pd.DataFrame()\n",
    "        return empty_df, sentences1, sentences2\n",
    "\n",
    "    def get_minhash(sentence):\n",
    "        minhash = MinHash()\n",
    "        for word in sentence:\n",
    "            minhash.update(word.encode('utf8'))\n",
    "        return minhash\n",
    "\n",
    "    minhashes1 = [get_minhash(sentence) for sentence in sentences1]\n",
    "    minhashes2 = [get_minhash(sentence) for sentence in sentences2]\n",
    "\n",
    "    similarity_matrix = [\n",
    "        [m1.jaccard(m2) for m2 in minhashes2]\n",
    "        for m1 in minhashes1\n",
    "    ]\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        similarity_matrix,\n",
    "        index=range(len(sentences1)),\n",
    "        columns=range(len(sentences2))\n",
    "    )\n",
    "\n",
    "    return df, sentences1, sentences2"
   ],
   "id": "dbb5ab37d44db89c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Combining Data and Functions",
   "id": "dd4e0274112c297b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data",
   "id": "789221d2a31ce5c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open('Iliad_lemmatized.txt', 'r', encoding=\"utf-8\") as f:\n",
    "    iliad = f.read()\n",
    "with open('Odyssey_lemmatized.txt', 'r', encoding=\"utf-8\") as f:\n",
    "    odyssey = f.read()"
   ],
   "id": "a2de80207c44c107",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from nltk.corpus import CategorizedPlaintextCorpusReader\n",
    "path = 'Lemmatized_Data'\n",
    "mycorpus = CategorizedPlaintextCorpusReader(path, r'.*\\.txt', cat_pattern=r'(.*?)_.*')"
   ],
   "id": "9d4006f422f6c62f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Iliad",
   "id": "e9d7773900364e21"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "columns = ['text1', 'text2', 'sentence1','sentence2','similarity']\n",
    "all_rows = []\n",
    "for f in mycorpus.fileids():\n",
    "    print(f\"Processing {f}...\")\n",
    "\n",
    "    text2 = ' '.join(mycorpus.words(fileids=f))\n",
    "    tfidf_df, sentences1, sentences2 = calculate_tfidf_similarity(iliad, text2)\n",
    "    pairs = get_sentence_pairs_above_threshold(tfidf_df, sentences1, sentences2, 0.5)\n",
    "\n",
    "    for pair in pairs:\n",
    "        all_rows.append(['Iliad', f, pair['sentence1'], pair['sentence2'], pair['similarity']])\n",
    "            \n",
    "df_tfidf = pd.DataFrame(all_rows, columns=columns)\n"
   ],
   "id": "38b52a7eacfed4e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_tfidf",
   "id": "be132625586e2ff1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "columns = ['text1', 'text2', 'sentence1','sentence2','similarity']\n",
    "all_rows = []\n",
    "for f in mycorpus.fileids():\n",
    "    print(f\"Processing {f}...\")\n",
    "\n",
    "    text2 = ' '.join(mycorpus.words(fileids=f))\n",
    "    fasttext_df, sentences1, sentences2 = calculate_fasttext_similarity(iliad, text2)\n",
    "    pairs = get_sentence_pairs_above_threshold(fasttext_df, sentences1, sentences2, 0.92)\n",
    "\n",
    "    for pair in pairs:\n",
    "        all_rows.append(['Iliad', f, pair['sentence1'], pair['sentence2'], pair['similarity']])\n",
    "            \n",
    "df_fasttext = pd.DataFrame(all_rows, columns=columns)"
   ],
   "id": "66a3a6ab57bc886",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_fasttext",
   "id": "a2af0fcaa396b24e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "columns = ['text1', 'text2', 'sentence1','sentence2','similarity']\n",
    "all_rows = []\n",
    "for f in mycorpus.fileids():\n",
    "    print(f\"Processing {f}...\")\n",
    "\n",
    "    text2 = ' '.join(mycorpus.words(fileids=f))\n",
    "    minhash_df, sentences1, sentences2 = calculate_minhash_similarity(iliad, text2)\n",
    "    pairs = get_sentence_pairs_above_threshold(minhash_df, sentences1, sentences2, 0.50)\n",
    "\n",
    "    for pair in pairs:\n",
    "        all_rows.append(['Iliad', f, pair['sentence1'], pair['sentence2'], pair['similarity']])\n",
    "            \n",
    "df_minhash = pd.DataFrame(all_rows, columns=columns)"
   ],
   "id": "35b5a8f2f1686686",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_minhash",
   "id": "b7b8b95add3ed0d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Odyssey",
   "id": "45633fe44ec75ce4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "columns = ['text1', 'text2', 'sentence1', 'sentence2', 'similarity']\n",
    "all_rows = []\n",
    "for f in mycorpus.fileids():\n",
    "    print(f\"Processing {f}...\")\n",
    "\n",
    "    text2 = ' '.join(mycorpus.words(fileids=f))\n",
    "    tfidf_df, sentences1, sentences2 = calculate_tfidf_similarity(iliad, text2)\n",
    "    pairs = get_sentence_pairs_above_threshold(tfidf_df, sentences1, sentences2, 0.5)\n",
    "\n",
    "    for pair in pairs:\n",
    "        all_rows.append(['Iliad', f, pair['sentence1'], pair['sentence2'], pair['similarity']])\n",
    "\n",
    "df_tfidf = pd.DataFrame(all_rows, columns=columns)\n",
    "df_tfidf"
   ],
   "id": "f211b156d7ba38a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "columns = ['text1', 'text2', 'sentence1','sentence2','similarity']\n",
    "all_rows = []\n",
    "for f in mycorpus.fileids():\n",
    "    print(f\"Processing {f}...\")\n",
    "\n",
    "    text2 = ' '.join(mycorpus.words(fileids=f))\n",
    "    fasttext_df, sentences1, sentences2 = calculate_fasttext_similarity(iliad, text2)\n",
    "    pairs = get_sentence_pairs_above_threshold(fasttext_df, sentences1, sentences2, 0.92)\n",
    "\n",
    "    for pair in pairs:\n",
    "        all_rows.append(['Iliad', f, pair['sentence1'], pair['sentence2'], pair['similarity']])\n",
    "            \n",
    "df_fasttext = pd.DataFrame(all_rows, columns=columns)\n",
    "df_fasttext"
   ],
   "id": "c5c703baffa2a361",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "columns = ['text1', 'text2', 'sentence1','sentence2','similarity']\n",
    "all_rows = []\n",
    "for f in mycorpus.fileids():\n",
    "    print(f\"Processing {f}...\")\n",
    "\n",
    "    text2 = ' '.join(mycorpus.words(fileids=f))\n",
    "    minhash_df, sentences1, sentences2 = calculate_minhash_similarity(iliad, text2)\n",
    "    pairs = get_sentence_pairs_above_threshold(minhash_df, sentences1, sentences2, 0.65)\n",
    "\n",
    "    for pair in pairs:\n",
    "        all_rows.append(['Iliad', f, pair['sentence1'], pair['sentence2'], pair['similarity']])\n",
    "            \n",
    "df_minhash = pd.DataFrame(all_rows, columns=columns)\n",
    "df_minhash"
   ],
   "id": "bdf29a8a9a5a2d5d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
